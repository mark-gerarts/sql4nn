{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "\n",
    "This notebook creates the necessary files to run the streamlit app (i.e. models\n",
    "and databases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, train the digit recognition CNN, saving every version of the model along\n",
    "the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import os\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import itertools\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64 // 4, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All models available\n"
     ]
    }
   ],
   "source": [
    "# Takes 5-6mins on a decent machine.\n",
    "def ensure_models_exist():\n",
    "    base_save_path = \"models/mnist_cnn_{epoch}.pt\"\n",
    "\n",
    "    # If the last one is saved, we assume all of them are\n",
    "    if os.path.exists(base_save_path.format(epoch=14)):\n",
    "        print(\"All models available\")\n",
    "        return\n",
    "\n",
    "    # These are the default values for the CLI app.\n",
    "    class args():\n",
    "        epochs = 14\n",
    "        torch.manual_seed(1)\n",
    "        device = torch.device(\"cpu\")\n",
    "        batch_size = 64\n",
    "        learning_rate = 1.0\n",
    "        gamma = 0.7\n",
    "        log_interval = 10\n",
    "        dry_run = False\n",
    "\n",
    "    args = args()\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                        transform=transform)\n",
    "    dataset2 = datasets.MNIST('../data', train=False,\n",
    "                        transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, batch_size=args.batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, batch_size=args.batch_size)\n",
    "\n",
    "    model = Net().to(args.device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "\n",
    "    torch.save(model.state_dict(), base_save_path.format(epoch=0))\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, args.device, train_loader, optimizer, epoch)\n",
    "        test(model, args.device, test_loader)\n",
    "        scheduler.step()\n",
    "        torch.save(model.state_dict(), base_save_path.format(epoch=epoch))\n",
    "\n",
    "ensure_models_exist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'll set up the multimodel database and save all models. This code is\n",
    "all taken from the \"Eval - CNN\" and the \"Eval - multiple networks\" notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "\n",
    "def batch_insert(con, generator, table, batch_size=8_000_000):\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(generator, batch_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "\n",
    "        df = pd.DataFrame(chunk)\n",
    "        con.execute(f\"INSERT INTO {table} SELECT * FROM df\")\n",
    "\n",
    "id = 0\n",
    "\n",
    "def get_contributing_points(output_point, kernel_size, stride=1, padding=0):\n",
    "    x_out, y_out = output_point\n",
    "    x_in_start = x_out * stride - padding\n",
    "    y_in_start = y_out * stride - padding\n",
    "\n",
    "    contributing_points = []\n",
    "    for i in range(kernel_size):\n",
    "        for j in range(kernel_size):\n",
    "            x_in = x_in_start + i\n",
    "            y_in = y_in_start + j\n",
    "            contributing_points.append(((x_in, y_in), (i, j)))\n",
    "\n",
    "    return contributing_points\n",
    "\n",
    "def load_model_into_db(model, name):\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "    result = con.execute(\n",
    "        \"INSERT INTO model (name) VALUES ($name) RETURNING (id)\",\n",
    "        {\"name\": name}\n",
    "    )\n",
    "\n",
    "    (model_id,) = result.fetchone()\n",
    "    # We have to hardcode this\n",
    "    input_size = 28\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "    _, conv1_kernel_size, _ = state_dict['conv1.weight'][0].size()\n",
    "\n",
    "    nodes = {}\n",
    "\n",
    "    conv1_weight = state_dict['conv1.weight']\n",
    "    conv1_bias = state_dict['conv1.bias']\n",
    "    conv1_output_size = input_size - conv1_kernel_size + 1\n",
    "\n",
    "    conv1_out_channels, conv1_in_channels, conv1_kernel_size, _ = conv1_weight.size()\n",
    "    if conv1_in_channels != 1:\n",
    "        raise Exception(\"Not handling >1 input channels for now\")\n",
    "\n",
    "    conv2_weight = state_dict['conv2.weight']\n",
    "    conv2_bias = state_dict['conv2.bias']\n",
    "\n",
    "    conv2_out_channels, conv2_in_channels, conv2_kernel_size, _ = conv2_weight.size()\n",
    "    conv2_output_size = conv1_output_size - conv2_kernel_size + 1\n",
    "\n",
    "    fc1_weight = state_dict['fc1.weight']\n",
    "    fc1_output_size, fc1_input_size = fc1_weight.size()\n",
    "\n",
    "    fc2_weight = state_dict['fc2.weight']\n",
    "    fc2_output_size, fc2_input_size = fc2_weight.size()\n",
    "\n",
    "    def node_generator(nodes):\n",
    "        (max_id_in_db,) = con.execute(\"SELECT COALESCE(MAX(id), 0) FROM node\").fetchone()\n",
    "        node_idx = max_id_in_db + 1\n",
    "\n",
    "        # Input nodes (1 channel for now)\n",
    "        for y in range(0, input_size):\n",
    "            for x in range(0, input_size):\n",
    "                name = f\"input.{x}.{y}\"\n",
    "                yield [node_idx, model_id, 0, name]\n",
    "                nodes[name] = node_idx\n",
    "                node_idx += 1\n",
    "\n",
    "        # Conv1\n",
    "        for y in range(0, conv1_output_size):\n",
    "            for x in range(0, conv1_output_size):\n",
    "                for c in range(0, conv1_out_channels):\n",
    "                    name = f\"conv1.{c}.{x}.{y}\"\n",
    "                    # The bias of this layer is simply the bias of the corresponding\n",
    "                    # kernel\n",
    "                    bias = conv1_bias[c]\n",
    "\n",
    "                    yield [node_idx, model_id, bias.item(), name]\n",
    "                    nodes[name] = node_idx\n",
    "                    node_idx += 1\n",
    "\n",
    "        # Conv2\n",
    "        for y in range(0, conv2_output_size):\n",
    "            for x in range(0, conv2_output_size):\n",
    "                for c in range(0, conv2_out_channels):\n",
    "                    name = f\"conv2.{c}.{x}.{y}\"\n",
    "                    bias = conv2_bias[c]\n",
    "\n",
    "                    yield [node_idx, model_id, bias.item(), name]\n",
    "                    nodes[name] = node_idx\n",
    "                    node_idx += 1\n",
    "\n",
    "        # fc1\n",
    "        for i in range(0, fc1_output_size):\n",
    "            name = f\"fc1.{i}\"\n",
    "            bias = state_dict['fc1.bias'][i]\n",
    "            yield [node_idx, model_id, bias.item(), name]\n",
    "            nodes[name] = node_idx\n",
    "            node_idx += 1\n",
    "\n",
    "        # fc2\n",
    "        for i in range(0, fc2_output_size):\n",
    "            name = f\"fc2.{i}\"\n",
    "            bias = state_dict['fc2.bias'][i]\n",
    "            yield [node_idx, model_id, bias.item(), name]\n",
    "            nodes[name] = node_idx\n",
    "            node_idx += 1\n",
    "\n",
    "    def edge_generator(nodes):\n",
    "        # Add the edges from input to conv1. Per channel, per output pixel of the\n",
    "        # convolution, we have to match the 9 input pixels to it (for a 3x3 kernel)\n",
    "        for c in range(0, conv1_out_channels):\n",
    "            for y_conv in range(0, conv1_output_size):\n",
    "                for x_conv in range(0, conv1_output_size):\n",
    "                    # (x_conv, y_conv) is the position in the output channel. We can\n",
    "                    # find the 9 matching input values from them.\n",
    "                    for (p_in, p_kernel) in get_contributing_points((x_conv, y_conv), conv1_kernel_size):\n",
    "                        (x_in, y_in) = p_in\n",
    "                        (x_kernel, y_kernel) = p_kernel\n",
    "\n",
    "                        # 0 corresponds to the input channel (which we only have one\n",
    "                        # of).\n",
    "                        kernel = conv1_weight[c][0]\n",
    "                        weight = kernel[y_kernel][x_kernel]\n",
    "\n",
    "                        src = nodes[f\"input.{x_in}.{y_in}\"]\n",
    "                        dst = nodes[f\"conv1.{c}.{x_conv}.{y_conv}\"]\n",
    "\n",
    "                        yield [model_id, src, dst, weight.item()]\n",
    "\n",
    "\n",
    "        # Add the edges from conv1 to conv2. This is similar as connecting the input to\n",
    "        # conv1, except that we have 2 input channels. Outputs are summed per output\n",
    "        # channel.\n",
    "        for c_out in range(0, conv2_out_channels):\n",
    "            for y_conv2 in range(0, conv2_output_size):\n",
    "                for x_conv2 in range(0, conv2_output_size):\n",
    "                    for (p_in, p_kernel) in get_contributing_points((x_conv2, y_conv2), conv2_kernel_size):\n",
    "                        (x_in, y_in) = p_in\n",
    "                        (x_kernel, y_kernel) = p_kernel\n",
    "\n",
    "                        for c_in in range(0, conv2_in_channels):\n",
    "                            kernel = conv2_weight[c_out][c_in]\n",
    "                            weight = kernel[y_kernel][x_kernel]\n",
    "\n",
    "                            src = nodes[f\"conv1.{c_in}.{x_in}.{y_in}\"]\n",
    "                            dst = nodes[f\"conv2.{c_out}.{x_conv2}.{y_conv2}\"]\n",
    "\n",
    "                            yield [model_id, src, dst, weight.item()]\n",
    "\n",
    "        # Connect conv2 to fc1.\n",
    "        for c in range(0, conv2_out_channels):\n",
    "            for y_conv in range(0, conv2_output_size):\n",
    "                for x_conv in range(0, conv2_output_size):\n",
    "                    for i in range(0, fc1_output_size):\n",
    "                        # By adding the channel offset, we flatten.\n",
    "                        channel_offset = c * conv2_output_size * conv2_output_size\n",
    "                        weight = fc1_weight[i][y_conv * conv2_output_size + x_conv + channel_offset]\n",
    "\n",
    "                        src = nodes[f\"conv2.{c}.{x_conv}.{y_conv}\"]\n",
    "                        dst = nodes[f\"fc1.{i}\"]\n",
    "\n",
    "                        yield [model_id, src, dst, weight.item()]\n",
    "\n",
    "        # Connect fc1 to fc2.\n",
    "        for i in range(0, fc2_input_size):\n",
    "            for j in range(0, fc2_output_size):\n",
    "                weight = fc2_weight[j][i]\n",
    "\n",
    "                src = nodes[f\"fc1.{i}\"]\n",
    "                dst = nodes[f\"fc2.{j}\"]\n",
    "\n",
    "                yield [model_id, src, dst, weight.item()]\n",
    "\n",
    "    batch_insert(con, node_generator(nodes), \"node\")\n",
    "    batch_insert(con, edge_generator(nodes), \"edge\")\n",
    "\n",
    "\n",
    "def create_db():\n",
    "    save_path = 'dbs/cnn_multimodel.db'\n",
    "    if os.path.exists(save_path):\n",
    "        return\n",
    "\n",
    "    con.execute(\"DROP TABLE IF EXISTS edge\")\n",
    "    con.execute(\"DROP TABLE IF EXISTS node\")\n",
    "    con.execute(\"DROP TABLE IF EXISTS model\")\n",
    "\n",
    "    con.execute(\"DROP SEQUENCE IF EXISTS seq_node\")\n",
    "    con.execute(\"CREATE SEQUENCE seq_node START 1\")\n",
    "\n",
    "    con.execute(\"DROP SEQUENCE IF EXISTS seq_model\")\n",
    "    con.execute(\"CREATE SEQUENCE seq_model START 1\")\n",
    "\n",
    "    con.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE model(\n",
    "            id INTEGER PRIMARY KEY DEFAULT NEXTVAL('seq_model'),\n",
    "            name TEXT\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "    con.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE node(\n",
    "            id INTEGER PRIMARY KEY DEFAULT NEXTVAL('seq_node'),\n",
    "            model_id INTEGER,\n",
    "            bias REAL,\n",
    "            name TEXT\n",
    "        )\"\"\"\n",
    "    )\n",
    "    con.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE edge(\n",
    "            model_id INTEGER,\n",
    "            src INTEGER,\n",
    "            dst INTEGER,\n",
    "            weight REAL\n",
    "        )\"\"\"\n",
    "    )\n",
    "    con.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE input(\n",
    "            input_set_id INTEGER,\n",
    "            input_node_idx INTEGER,\n",
    "            input_value REAL\n",
    "        )\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "    for epoch in range(0, 15):\n",
    "        model_path = f\"models/mnist_cnn_{epoch}.pt\"\n",
    "        model = Net()\n",
    "        model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "        load_model_into_db(model, f\"Epoch {epoch}\")\n",
    "\n",
    "    con.execute(f\"EXPORT DATABASE '{save_path}'\")\n",
    "\n",
    "create_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a separate database that only holds the single final\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "\n",
    "def load_or_create_database(model):\n",
    "    save_path = \"dbs/cnn_single.db\"\n",
    "    if os.path.exists(save_path):\n",
    "        return\n",
    "\n",
    "    con.execute(\"DROP TABLE IF EXISTS edge\")\n",
    "    con.execute(\"DROP TABLE IF EXISTS node\")\n",
    "    con.execute(\"DROP SEQUENCE IF EXISTS seq_node\")\n",
    "    con.execute(\"DROP TABLE IF EXISTS input\")\n",
    "\n",
    "    con.execute(\"CREATE SEQUENCE seq_node START 1\")\n",
    "    con.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE node(\n",
    "            id INTEGER PRIMARY KEY DEFAULT NEXTVAL('seq_node'),\n",
    "            bias REAL,\n",
    "            name TEXT\n",
    "        )\"\"\"\n",
    "    )\n",
    "    con.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE edge(\n",
    "            src INTEGER,\n",
    "            dst INTEGER,\n",
    "            weight REAL\n",
    "        )\"\"\"\n",
    "    )\n",
    "\n",
    "    con.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE input(\n",
    "            input_set_id INTEGER,\n",
    "            input_node_idx INTEGER,\n",
    "            input_value REAL\n",
    "        )\"\"\"\n",
    "    )\n",
    "\n",
    "    # We have to hardcode this\n",
    "    input_size = 28\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "    _, conv1_kernel_size, _ = state_dict['conv1.weight'][0].size()\n",
    "\n",
    "    node_idx = 1\n",
    "    nodes = {}\n",
    "\n",
    "    conv1_weight = state_dict['conv1.weight']\n",
    "    conv1_bias = state_dict['conv1.bias']\n",
    "    conv1_output_size = input_size - conv1_kernel_size + 1\n",
    "\n",
    "    conv1_out_channels, conv1_in_channels, conv1_kernel_size, _ = conv1_weight.size()\n",
    "    if conv1_in_channels != 1:\n",
    "        raise Exception(\"Not handling >1 input channels for now\")\n",
    "\n",
    "    conv2_weight = state_dict['conv2.weight']\n",
    "    conv2_bias = state_dict['conv2.bias']\n",
    "\n",
    "    conv2_out_channels, conv2_in_channels, conv2_kernel_size, _ = conv2_weight.size()\n",
    "    conv2_output_size = conv1_output_size - conv2_kernel_size + 1\n",
    "\n",
    "    fc1_weight = state_dict['fc1.weight']\n",
    "    fc1_output_size, fc1_input_size = fc1_weight.size()\n",
    "\n",
    "    fc2_weight = state_dict['fc2.weight']\n",
    "    fc2_output_size, fc2_input_size = fc2_weight.size()\n",
    "\n",
    "    def node_generator(nodes):\n",
    "        node_idx = 1\n",
    "\n",
    "        # Input nodes (1 channel for now)\n",
    "        for y in range(0, input_size):\n",
    "            for x in range(0, input_size):\n",
    "                name = f\"input.{x}.{y}\"\n",
    "                yield [node_idx, 0, name]\n",
    "                nodes[name] = node_idx\n",
    "                node_idx += 1\n",
    "\n",
    "        # Conv1\n",
    "        for y in range(0, conv1_output_size):\n",
    "            for x in range(0, conv1_output_size):\n",
    "                for c in range(0, conv1_out_channels):\n",
    "                    name = f\"conv1.{c}.{x}.{y}\"\n",
    "                    # The bias of this layer is simply the bias of the corresponding\n",
    "                    # kernel\n",
    "                    bias = conv1_bias[c]\n",
    "\n",
    "                    yield [node_idx, bias.item(), name]\n",
    "                    nodes[name] = node_idx\n",
    "                    node_idx += 1\n",
    "\n",
    "        # Conv2\n",
    "        for y in range(0, conv2_output_size):\n",
    "            for x in range(0, conv2_output_size):\n",
    "                for c in range(0, conv2_out_channels):\n",
    "                    name = f\"conv2.{c}.{x}.{y}\"\n",
    "                    bias = conv2_bias[c]\n",
    "\n",
    "                    yield [node_idx, bias.item(), name]\n",
    "                    nodes[name] = node_idx\n",
    "                    node_idx += 1\n",
    "\n",
    "        # fc1\n",
    "        for i in range(0, fc1_output_size):\n",
    "            name = f\"fc1.{i}\"\n",
    "            bias = state_dict['fc1.bias'][i]\n",
    "            yield [node_idx, bias.item(), name]\n",
    "            nodes[name] = node_idx\n",
    "            node_idx += 1\n",
    "\n",
    "        # fc2\n",
    "        for i in range(0, fc2_output_size):\n",
    "            name = f\"fc2.{i}\"\n",
    "            bias = state_dict['fc2.bias'][i]\n",
    "            yield [node_idx, bias.item(), name]\n",
    "            nodes[name] = node_idx\n",
    "            node_idx += 1\n",
    "\n",
    "    def edge_generator(nodes):\n",
    "        # Add the edges from input to conv1. Per channel, per output pixel of the\n",
    "        # convolution, we have to match the 9 input pixels to it (for a 3x3 kernel)\n",
    "        for c in range(0, conv1_out_channels):\n",
    "            for y_conv in range(0, conv1_output_size):\n",
    "                for x_conv in range(0, conv1_output_size):\n",
    "                    # (x_conv, y_conv) is the position in the output channel. We can\n",
    "                    # find the 9 matching input values from them.\n",
    "                    for (p_in, p_kernel) in get_contributing_points((x_conv, y_conv), conv1_kernel_size):\n",
    "                        (x_in, y_in) = p_in\n",
    "                        (x_kernel, y_kernel) = p_kernel\n",
    "\n",
    "                        # 0 corresponds to the input channel (which we only have one\n",
    "                        # of).\n",
    "                        kernel = conv1_weight[c][0]\n",
    "                        weight = kernel[y_kernel][x_kernel]\n",
    "\n",
    "                        src = nodes[f\"input.{x_in}.{y_in}\"]\n",
    "                        dst = nodes[f\"conv1.{c}.{x_conv}.{y_conv}\"]\n",
    "\n",
    "                        yield [src, dst, weight.item()]\n",
    "\n",
    "\n",
    "        # Add the edges from conv1 to conv2. This is similar as connecting the input to\n",
    "        # conv1, except that we have 2 input channels. Outputs are summed per output\n",
    "        # channel.\n",
    "        for c_out in range(0, conv2_out_channels):\n",
    "            for y_conv2 in range(0, conv2_output_size):\n",
    "                for x_conv2 in range(0, conv2_output_size):\n",
    "                    for (p_in, p_kernel) in get_contributing_points((x_conv2, y_conv2), conv2_kernel_size):\n",
    "                        (x_in, y_in) = p_in\n",
    "                        (x_kernel, y_kernel) = p_kernel\n",
    "\n",
    "                        for c_in in range(0, conv2_in_channels):\n",
    "                            kernel = conv2_weight[c_out][c_in]\n",
    "                            weight = kernel[y_kernel][x_kernel]\n",
    "\n",
    "                            src = nodes[f\"conv1.{c_in}.{x_in}.{y_in}\"]\n",
    "                            dst = nodes[f\"conv2.{c_out}.{x_conv2}.{y_conv2}\"]\n",
    "\n",
    "                            yield [src, dst, weight.item()]\n",
    "\n",
    "        # Connect conv2 to fc1.\n",
    "        for c in range(0, conv2_out_channels):\n",
    "            for y_conv in range(0, conv2_output_size):\n",
    "                for x_conv in range(0, conv2_output_size):\n",
    "                    for i in range(0, fc1_output_size):\n",
    "                        # By adding the channel offset, we flatten.\n",
    "                        channel_offset = c * conv2_output_size * conv2_output_size\n",
    "                        weight = fc1_weight[i][y_conv * conv2_output_size + x_conv + channel_offset]\n",
    "\n",
    "                        src = nodes[f\"conv2.{c}.{x_conv}.{y_conv}\"]\n",
    "                        dst = nodes[f\"fc1.{i}\"]\n",
    "\n",
    "                        yield [src, dst, weight.item()]\n",
    "\n",
    "        # Connect fc1 to fc2.\n",
    "        for i in range(0, fc2_input_size):\n",
    "            for j in range(0, fc2_output_size):\n",
    "                weight = fc2_weight[j][i]\n",
    "\n",
    "                src = nodes[f\"fc1.{i}\"]\n",
    "                dst = nodes[f\"fc2.{j}\"]\n",
    "\n",
    "                yield [src, dst, weight.item()]\n",
    "\n",
    "    batch_insert(con, node_generator(nodes), \"node\")\n",
    "    batch_insert(con, edge_generator(nodes), \"edge\")\n",
    "\n",
    "    con.execute(f\"EXPORT DATABASE '{save_path}'\")\n",
    "\n",
    "model_path = f\"models/mnist_cnn_14.pt\"\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "load_or_create_database(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do something similar: we train multiple models, with ever decreasing\n",
    "size. This way we can check if smaller models still yield good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShrinkingNet(nn.Module):\n",
    "    def __init__(self, shrink_factor=0):\n",
    "        super(ShrinkingNet, self).__init__()\n",
    "        scale = 2 ** shrink_factor\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32 // scale, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32 // scale, 64 // (scale * 4), 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216 // scale, 128 // scale)\n",
    "        self.fc2 = nn.Linear(128 // scale, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "for i in range(0, 5):\n",
    "    base_save_path = \"models/mnist_cnn_shrinking_{i}.pt\"\n",
    "    save_path = base_save_path.format(i=i)\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"Training with shrinkfactor {i}\")\n",
    "\n",
    "    class args():\n",
    "        epochs = 14\n",
    "        torch.manual_seed(1)\n",
    "        device = torch.device(\"cpu\")\n",
    "        batch_size = 64\n",
    "        learning_rate = 1.0\n",
    "        gamma = 0.7\n",
    "        log_interval = 10\n",
    "        dry_run = False\n",
    "\n",
    "    args = args()\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                        transform=transform)\n",
    "    dataset2 = datasets.MNIST('../data', train=False,\n",
    "                        transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, batch_size=args.batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, batch_size=args.batch_size)\n",
    "\n",
    "    model = ShrinkingNet(i).to(args.device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, args.device, train_loader, optimizer, epoch)\n",
    "        test(model, args.device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    torch.save(model.state_dict(), save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db():\n",
    "    save_path = 'dbs/cnn_multimodel_size.db'\n",
    "    if os.path.exists(save_path):\n",
    "        return\n",
    "\n",
    "    con.execute(\"DROP TABLE IF EXISTS edge\")\n",
    "    con.execute(\"DROP TABLE IF EXISTS node\")\n",
    "    con.execute(\"DROP TABLE IF EXISTS model\")\n",
    "\n",
    "    con.execute(\"DROP SEQUENCE IF EXISTS seq_node\")\n",
    "    con.execute(\"CREATE SEQUENCE seq_node START 1\")\n",
    "\n",
    "    con.execute(\"DROP SEQUENCE IF EXISTS seq_model\")\n",
    "    con.execute(\"CREATE SEQUENCE seq_model START 1\")\n",
    "\n",
    "    con.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE model(\n",
    "            id INTEGER PRIMARY KEY DEFAULT NEXTVAL('seq_model'),\n",
    "            name TEXT\n",
    "        )\n",
    "        \"\"\"\n",
    "    )\n",
    "    con.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE node(\n",
    "            id INTEGER PRIMARY KEY DEFAULT NEXTVAL('seq_node'),\n",
    "            model_id INTEGER,\n",
    "            bias REAL,\n",
    "            name TEXT\n",
    "        )\"\"\"\n",
    "    )\n",
    "    con.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE edge(\n",
    "            model_id INTEGER,\n",
    "            src INTEGER,\n",
    "            dst INTEGER,\n",
    "            weight REAL\n",
    "        )\"\"\"\n",
    "    )\n",
    "    con.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE input(\n",
    "            input_set_id INTEGER,\n",
    "            input_node_idx INTEGER,\n",
    "            input_value REAL\n",
    "        )\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "    for i in range(0, 5):\n",
    "        model_path = f\"models/mnist_cnn_shrinking_{i}.pt\"\n",
    "        model = ShrinkingNet(i)\n",
    "        model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "        labels = [\"Regular\", \"2x smaller\", \"4x smaller\", \"8x smaller\", \"16x smaller\"]\n",
    "        load_model_into_db(model, labels[i])\n",
    "\n",
    "    con.execute(f\"EXPORT DATABASE '{save_path}'\")\n",
    "\n",
    "create_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import os\n",
    "\n",
    "\n",
    "class ReLUFNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size=1, hidden_size=4, num_hidden_layers=10, output_size=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "\n",
    "        layers.append(nn.Linear(input_size, hidden_size))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "\n",
    "def train(model, x_train, y_train, epochs=1000, save_path=None):\n",
    "    if save_path and os.path.exists(save_path):\n",
    "        model.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "        return\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    x_train_tensor = ensure_tensor(x_train).unsqueeze(1)\n",
    "    y_train_tensor = ensure_tensor(y_train).unsqueeze(1)\n",
    "\n",
    "    num_epochs = epochs\n",
    "    for _ in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_train_tensor)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if save_path:\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "\n",
    "def ensure_tensor(tensor_or_array):\n",
    "    if torch.is_tensor(tensor_or_array):\n",
    "        return tensor_or_array\n",
    "    else:\n",
    "        return torch.tensor(tensor_or_array, dtype=torch.float32)\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "def _initialize_database(save_path=None):\n",
    "    if save_path and os.path.isdir(save_path):\n",
    "        shutil.rmtree(save_path)\n",
    "\n",
    "    con.execute(\"DROP TABLE IF EXISTS edge\")\n",
    "    con.execute(\"DROP TABLE IF EXISTS node\")\n",
    "    con.execute(\"DROP SEQUENCE IF EXISTS seq_node\")\n",
    "    con.execute(\"DROP TABLE IF EXISTS input\")\n",
    "\n",
    "    con.execute(\"CREATE SEQUENCE seq_node START 1\")\n",
    "    con.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE node(\n",
    "            id INTEGER PRIMARY KEY DEFAULT NEXTVAL('seq_node'),\n",
    "            bias REAL,\n",
    "            name TEXT\n",
    "        )\"\"\"\n",
    "    )\n",
    "    # Foreign keys are omitted for performance.\n",
    "    con.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE edge(\n",
    "            src INTEGER,\n",
    "            dst INTEGER,\n",
    "            weight REAL\n",
    "        )\"\"\"\n",
    "    )\n",
    "\n",
    "    con.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE input(\n",
    "            input_set_id INTEGER,\n",
    "            input_node_idx INTEGER,\n",
    "            input_value REAL\n",
    "        )\"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def load_pytorch_model_into_db(model, save_path=None):\n",
    "    return load_state_dict_into_db(model.state_dict(), save_path)\n",
    "\n",
    "\n",
    "def batch_insert(generator, table, batch_size=8_000_000):\n",
    "    \"\"\"\n",
    "    Inserts data in batches into duckdb, to find a middle ground between\n",
    "    performance and memory consumption. A batch size of 10M consumes ~4GB RAM.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(generator, batch_size))\n",
    "        if not chunk:\n",
    "            break\n",
    "\n",
    "        df = pd.DataFrame(chunk)\n",
    "        con.execute(f\"INSERT INTO {table} SELECT * FROM df\")\n",
    "\n",
    "\n",
    "def load_state_dict_into_db(state_dict, save_path=None):\n",
    "    _initialize_database(save_path)\n",
    "\n",
    "    # We keep the node IDs per layer in memory so we can insert the edges later on.\n",
    "    node_ids = [[]]\n",
    "\n",
    "    def nodes():\n",
    "        # First, insert the input nodes.\n",
    "\n",
    "        # Retrieves the input x weights matrix\n",
    "        input_weights = list(state_dict.items())[0][1].tolist()\n",
    "        num_input_nodes = len(input_weights[0])\n",
    "\n",
    "        id = 0\n",
    "        for i in range(0, num_input_nodes):\n",
    "            id += 1\n",
    "            yield [id, 0, f\"input.{i}\"]\n",
    "            node_ids[0].append(id)\n",
    "\n",
    "        layer = 0\n",
    "        # In the first pass, insert all nodes with their biases\n",
    "        for name, values in state_dict.items():\n",
    "            # state_dict alternates between weight and bias tensors.\n",
    "            if not \"bias\" in name:\n",
    "                continue\n",
    "\n",
    "            node_ids.append([])\n",
    "\n",
    "            layer += 1\n",
    "            for i, bias in enumerate(values.tolist()):\n",
    "                id += 1\n",
    "                yield [id, bias, f\"{name}.{i}\"]\n",
    "                node_ids[layer].append(id)\n",
    "\n",
    "    def edges():\n",
    "        # In the second pass, insert all edges and their weights. This assumes a fully\n",
    "        # connected network.\n",
    "        layer = 0\n",
    "        for name, values in state_dict.items():\n",
    "            # state_dict alternates between weight and bias tensors.\n",
    "            if not \"weight\" in name:\n",
    "                continue\n",
    "\n",
    "            # Each weight tensor has a list for each node in the next layer. The\n",
    "            # elements of this list correspond to the nodes of the current layer.\n",
    "            weight_tensor = values.tolist()\n",
    "            for from_index, from_node in enumerate(node_ids[layer]):\n",
    "                for to_index, to_node in enumerate(node_ids[layer + 1]):\n",
    "                    weight = weight_tensor[to_index][from_index]\n",
    "                    yield [from_node, to_node, weight]\n",
    "\n",
    "            layer += 1\n",
    "\n",
    "    batch_insert(nodes(), \"node\")\n",
    "    batch_insert(edges(), \"edge\")\n",
    "\n",
    "    if save_path:\n",
    "        con.execute(f\"EXPORT DATABASE '{save_path}'\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Add a manual seed for reproducibility.\n",
    "torch.manual_seed(223)\n",
    "\n",
    "# Define a simple function to train the network on.\n",
    "def f(x):\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    elif 0 <= x < 5:\n",
    "        return x\n",
    "    elif 5 <= x < 10:\n",
    "        return 10-x\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# The function only does interesting stuff between x=0 and x=10, so limit the\n",
    "# training data to that area.\n",
    "x_train = np.linspace(-5, 15, 1000)\n",
    "y_train = np.array([f(x) for x in x_train])\n",
    "\n",
    "model = ReLUFNN(input_size=1, hidden_size=2, num_hidden_layers=2, output_size=1)\n",
    "train(model, x_train, y_train, save_path=\"models/basic_eval.pt\")\n",
    "load_pytorch_model_into_db(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PWL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_train = np.linspace(-2*math.pi, 2*math.pi, 10000)\n",
    "y_train = np.array([math.sin(x) for x in x_train])\n",
    "\n",
    "model = ReLUFNN(input_size=1, output_size=1, hidden_size=1000, num_hidden_layers=1)\n",
    "train(model, x_train, y_train, epochs=750, save_path=\"models/pwl_geometric_sine.pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted = model(torch.tensor(x_train, dtype=torch.float32).unsqueeze(1)).detach().numpy()\n",
    "\n",
    "load_pytorch_model_into_db(model, save_path=\"dbs/pwl_geometric_sine.db\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
