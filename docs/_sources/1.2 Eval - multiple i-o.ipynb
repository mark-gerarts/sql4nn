{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple input and output neurons\n",
    "\n",
    "In the [previous chapter](./1.1%20Eval%20-%20basic%20eval.ipynb) we constructed\n",
    "a SQL query for a basic neural network that only had a single input and output\n",
    "neuron. We'll now adapt this query to be able to handle multiple input and/or\n",
    "output neurons. The network can have any number of hidden layers, but we\n",
    "assume we know how many of them beforehand.\n",
    "\n",
    "## Creating the model\n",
    "\n",
    "Let's start of by creating and training a network that accepts two input values,\n",
    "and predicts two output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import utils.sqlite as db\n",
    "import utils.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(223)\n",
    "\n",
    "# The function we're going to learn accepts 2 input values and returns 2 output\n",
    "# values.\n",
    "def f(x, y):\n",
    "    return [2*x, 4*y]\n",
    "\n",
    "num_samples = 100\n",
    "x_train = torch.randn(num_samples, 2) * 100\n",
    "y_train = [f(x,y) for [x,y] in x_train]\n",
    "\n",
    "model = nn.ReLUFNN(input_size=2, output_size=2, hidden_size=2, num_hidden_layers=1)\n",
    "nn.train(model, x_train, y_train, save_path=\"models/eval_multiple_io.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's load the trained model into the database again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.load_pytorch_model_into_db(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the query\n",
    "\n",
    "Let's create the eval query. We first give it in full again, and then\n",
    "go over each part separately.\n",
    "\n",
    "```sql\n",
    "WITH input_values AS (\n",
    "    SELECT 1 as input_node_idx, ? as input_value \n",
    "    UNION \n",
    "    SELECT 2 as input_node_idx, ? as input_value\n",
    "),\n",
    "input_nodes AS (\n",
    "    SELECT\n",
    "        id,\n",
    "        bias,\n",
    "        ROW_NUMBER() OVER (ORDER BY id) AS input_node_idx\n",
    "    FROM node\n",
    "    WHERE id NOT IN\n",
    "    (\n",
    "        SELECT dst FROM edge\n",
    "    )\n",
    "),\n",
    "t1 AS (\n",
    "    SELECT\n",
    "        MAX(\n",
    "            0,\n",
    "            n.bias + SUM(e.weight * v.input_value)\n",
    "        ) AS t1,\n",
    "        e.dst AS id\n",
    "    FROM edge e\n",
    "    JOIN input_nodes i ON i.id = e.src\n",
    "    JOIN node n ON e.dst = n.id\n",
    "    JOIN input_values v ON i.input_node_idx = v.input_node_idx\n",
    "    GROUP BY e.dst, n.bias\n",
    "),\n",
    "outputs AS (\n",
    "    SELECT\n",
    "        n.bias + SUM(e.weight * t1.t1) AS output_value,\n",
    "        e.dst AS output_node_id\n",
    "    FROM edge e\n",
    "    JOIN t1 ON t1.id = e.src\n",
    "    JOIN node n ON e.dst = n.id\n",
    "    GROUP BY e.dst, n.bias\n",
    ")\n",
    "SELECT * FROM outputs ORDER BY output_node_id;\n",
    "```\n",
    "\n",
    "Note that the bulk of the query is similar to [the one from the first\n",
    "chapter](./3.1.%20generic%20eval.ipynb). Let's go over the differences one by\n",
    "one.\n",
    "\n",
    "```sql\n",
    "WITH input_values AS (\n",
    "    SELECT 1 as input_node_idx, ? as input_value \n",
    "    UNION \n",
    "    SELECT 2 as input_node_idx, ? as input_value\n",
    ")\n",
    "```\n",
    "\n",
    "Now that we have multiple inputs, we manually pass them as a union of hardcoded\n",
    "values. We also explicitly state which input neuron each value belongs to. Note\n",
    "that it wouldn't be very difficult to instead read input values from an existing\n",
    "table.\n",
    "\n",
    "```sql\n",
    "input_nodes AS (\n",
    "    SELECT\n",
    "        id,\n",
    "        bias,\n",
    "        ROW_NUMBER() OVER (ORDER BY id) AS input_node_idx\n",
    "    FROM node\n",
    "    WHERE id NOT IN\n",
    "    (\n",
    "        SELECT dst FROM edge\n",
    "    )\n",
    ")\n",
    "```\n",
    "\n",
    "We still select the input nodes by checking which nodes do not have an incoming\n",
    "edge. However, we also use SQL's `ROW_NUMBER` functionality to assign a numeric\n",
    "index to each input node. This way, we can match them to the corresponding input\n",
    "value. This is done by the following join in the $t_1$ term:\n",
    "\n",
    "```sql\n",
    "JOIN input_values v ON i.input_node_idx = v.input_node_idx\n",
    "```\n",
    "\n",
    "The output CTE is identical, with one addition: we add an explicit `ORDER\n",
    "BY`-clause so that the output is always in the expected order.\n",
    "\n",
    "The following python code constructs a query for a given neural network,\n",
    "dynamically creating the input statements and inductive terms for each hidden\n",
    "layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_nn(model, input_value):\n",
    "    num_layers = int(len(model.state_dict()) / 2)\n",
    "\n",
    "    # Add as many input clauses as needed.\n",
    "    input_clauses = []\n",
    "    for i,_ in enumerate(input_value):\n",
    "        input_clauses.append(f\"SELECT {i + 1} as input_node_idx, ? as input_value\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "        WITH input_values AS (\n",
    "            {\" UNION \".join(input_clauses)}\n",
    "        ),\n",
    "        input_nodes AS (\n",
    "            SELECT\n",
    "                id,\n",
    "                bias,\n",
    "                ROW_NUMBER() OVER (ORDER BY id) AS input_node_idx\n",
    "            FROM node\n",
    "            WHERE id NOT IN\n",
    "            (\n",
    "                SELECT dst FROM edge\n",
    "            )\n",
    "        ),\n",
    "        t1 AS (\n",
    "            SELECT\n",
    "                MAX(\n",
    "                    0,\n",
    "                    n.bias + SUM(e.weight * v.input_value)\n",
    "                ) AS t1,\n",
    "                e.dst AS id\n",
    "            FROM edge e\n",
    "            JOIN input_nodes i ON i.id = e.src\n",
    "            JOIN node n ON e.dst = n.id\n",
    "            JOIN input_values v ON i.input_node_idx = v.input_node_idx\n",
    "            GROUP BY e.dst, n.bias\n",
    "        ),\n",
    "        \"\"\"\n",
    "\n",
    "    # Everything else is the same as in the previous part.\n",
    "    for hidden_layer in range(2, num_layers):\n",
    "        curr = hidden_layer\n",
    "        prev = hidden_layer - 1\n",
    "        query += f\"\"\"\n",
    "            t{curr} AS (\n",
    "                SELECT\n",
    "                    MAX(\n",
    "                        0,\n",
    "                        n.bias + SUM(e.weight * t{prev}.t{prev})\n",
    "                    ) AS t{curr},\n",
    "                    e.dst AS id\n",
    "                FROM edge e\n",
    "                JOIN t{prev} ON t{prev}.id = e.src\n",
    "                JOIN node n ON e.dst = n.id\n",
    "                GROUP BY e.dst, n.bias\n",
    "            ),\n",
    "        \"\"\"\n",
    "\n",
    "    prev = num_layers - 1\n",
    "    query += f\"\"\"\n",
    "        outputs AS (\n",
    "            SELECT\n",
    "                n.bias + SUM(e.weight * t{prev}.t{prev}) AS output_value,\n",
    "                e.dst AS output_node_id\n",
    "            FROM edge e\n",
    "            JOIN t{prev} ON t{prev}.id = e.src\n",
    "            JOIN node n ON e.dst = n.id\n",
    "            GROUP BY e.dst, n.bias\n",
    "        )\n",
    "        SELECT * FROM outputs ORDER BY output_node_id;\n",
    "    \"\"\"\n",
    "\n",
    "    results = db.con.execute(query, input_value).fetchall()\n",
    "\n",
    "    return [result[0] for result in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our query on the model we created and compare the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The neural network predicted [-24.508732  54.89358 ]\n",
      "The SQL query calculated [-24.508734581785873, 54.89358546175163]\n"
     ]
    }
   ],
   "source": [
    "nn_output = model(torch.tensor([5,20], dtype=torch.float32)).detach().numpy()\n",
    "sql_output = eval_nn(model, [5, 20])\n",
    "\n",
    "print(f\"The neural network predicted {nn_output}\")\n",
    "print(f\"The SQL query calculated {sql_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's do the same thing for a larger amount of input samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average difference for output value 1 is 3.84342975134011e-06\n",
      "The average difference for output value 2 is 1.4083713261818786e-05\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['in_1', 'in_2', 'nn_out_1', 'nn_out_2', 'sql_out_1', 'sql_out_2'])\n",
    "\n",
    "for i, input_vals in enumerate(x_train):\n",
    "    [nn_out_1, nn_out_2] = model(input_vals).detach().numpy()\n",
    "    [sql_out_1, sql_out_2] = eval_nn(model, input_vals.tolist())\n",
    "    df.loc[i] = [\n",
    "        input_vals[0].item(), input_vals[1].item(),\n",
    "        nn_out_1, nn_out_2,\n",
    "        sql_out_1, sql_out_2\n",
    "    ]\n",
    "\n",
    "delta_1_avg = (abs(df['nn_out_1'] - df['sql_out_1'])).mean()\n",
    "delta_2_avg = (abs(df['nn_out_2'] - df['sql_out_2'])).mean()\n",
    "\n",
    "print(f\"The average difference for output value 1 is {delta_1_avg}\")\n",
    "print(f\"The average difference for output value 2 is {delta_2_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the outputs are again identical, barring some floating point\n",
    "errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more hidden layers\n",
    "\n",
    "We now have a generic `eval` that works with multiple input/output neurons.\n",
    "Let's also check how it performs with more hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_model = nn.ReLUFNN(input_size=2, output_size=2, hidden_size=5, num_hidden_layers=50)\n",
    "nn.train(bigger_model, x_train, y_train, save_path=\"models/eval_multiple_io_bigger.pt\")\n",
    "db.load_pytorch_model_into_db(bigger_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-33.27764  -32.772896]\n",
      "[-33.27763928976887, -32.77289402078124]\n"
     ]
    }
   ],
   "source": [
    "nn_output = bigger_model(torch.tensor([5,20], dtype=torch.float32)).detach().numpy()\n",
    "sql_output = eval_nn(bigger_model, [5, 20])\n",
    "\n",
    "print(nn_output)\n",
    "print(sql_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average difference for output value 1 is 2.006617847882808e-06\n",
      "The average difference for output value 2 is 1.7922070441045435e-06\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['in_1', 'in_2', 'nn_out_1', 'nn_out_2', 'sql_out_1', 'sql_out_2'])\n",
    "\n",
    "for i, input_vals in enumerate(x_train):\n",
    "    [nn_out_1, nn_out_2] = bigger_model(input_vals).detach().numpy()\n",
    "    [sql_out_1, sql_out_2] = eval_nn(bigger_model, input_vals.tolist())\n",
    "    df.loc[i] = [\n",
    "        input_vals[0].item(), input_vals[1].item(),\n",
    "        nn_out_1, nn_out_2,\n",
    "        sql_out_1, sql_out_2\n",
    "    ]\n",
    "\n",
    "delta_1_avg = (abs(df['nn_out_1'] - df['sql_out_1'])).mean()\n",
    "delta_2_avg = (abs(df['nn_out_2'] - df['sql_out_2'])).mean()\n",
    "\n",
    "print(f\"The average difference for output value 1 is {delta_1_avg}\")\n",
    "print(f\"The average difference for output value 2 is {delta_2_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our query gives near identical result. \n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Where we first only allowed one input and one output neuron, we can now evaluate\n",
    "neural networks with any number of input and output neurons. In the next\n",
    "chapter, we'll go one step further and add the possibility to evaluate multiple\n",
    "input sets at the same time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
