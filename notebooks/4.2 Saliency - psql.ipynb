{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saliency in Postgres\n",
    "\n",
    "Since DuckDB's performance is not good for the saliency query, let's check if it\n",
    "is any better in postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import utils.duckdb as db\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64 // 4, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load('models/mnist_cnn.pt', weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to have a psql instance running:\n",
    "\n",
    "```\n",
    "docker run --rm --name postgres -p 5432:5432 -e POSTGRES_PASSWORD=pass postgres\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "con = psycopg2.connect(database=\"postgres\",\n",
    "                        host=\"127.0.0.1\",\n",
    "                        user=\"postgres\",\n",
    "                        password=\"pass\",\n",
    "                        port=\"5432\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor = con.cursor()\n",
    "cursor.execute(\"SELECT 1+1\")\n",
    "cursor.fetchone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, now we'll adapt the duckdb import script to postgres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contributing_points(output_point, kernel_size, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    Given an output point (of the image after convolution), calculates which\n",
    "    points of the input image contributed. They are returned, along with the\n",
    "    corresponding kernel location.\n",
    "    \"\"\"\n",
    "\n",
    "    x_out, y_out = output_point\n",
    "    x_in_start = x_out * stride - padding\n",
    "    y_in_start = y_out * stride - padding\n",
    "\n",
    "    contributing_points = []\n",
    "    for i in range(kernel_size):\n",
    "        for j in range(kernel_size):\n",
    "            x_in = x_in_start + i\n",
    "            y_in = y_in_start + j\n",
    "            contributing_points.append(((x_in, y_in), (i, j)))\n",
    "\n",
    "    return contributing_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import csv\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "cursor.close()\n",
    "cursor = con.cursor()\n",
    "\n",
    "cursor.execute(\"DROP TABLE IF EXISTS edge\")\n",
    "cursor.execute(\"DROP TABLE IF EXISTS node\")\n",
    "cursor.execute(\"DROP TABLE IF EXISTS input\")\n",
    "\n",
    "cursor.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE node(\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        bias REAL,\n",
    "        name TEXT\n",
    "    )\"\"\"\n",
    ")\n",
    "cursor.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE edge(\n",
    "        src INTEGER,\n",
    "        dst INTEGER,\n",
    "        weight REAL\n",
    "    )\"\"\"\n",
    ")\n",
    "cursor.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE input(\n",
    "        input_set_id INTEGER,\n",
    "        input_node_idx INTEGER,\n",
    "        input_value REAL\n",
    "    )\"\"\"\n",
    ")\n",
    "\n",
    "# We have to hardcode this\n",
    "input_size = 28\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "_, conv1_kernel_size, _ = state_dict['conv1.weight'][0].size()\n",
    "\n",
    "node_idx = 0\n",
    "nodes = {}\n",
    "\n",
    "conv1_weight = state_dict['conv1.weight']\n",
    "conv1_bias = state_dict['conv1.bias']\n",
    "conv1_output_size = input_size - conv1_kernel_size + 1\n",
    "\n",
    "conv1_out_channels, conv1_in_channels, conv1_kernel_size, _ = conv1_weight.size()\n",
    "if conv1_in_channels != 1:\n",
    "    raise Exception(\"Not handling >1 input channels for now\")\n",
    "\n",
    "conv2_weight = state_dict['conv2.weight']\n",
    "conv2_bias = state_dict['conv2.bias']\n",
    "\n",
    "conv2_out_channels, conv2_in_channels, conv2_kernel_size, _ = conv2_weight.size()\n",
    "conv2_output_size = conv1_output_size - conv2_kernel_size + 1\n",
    "\n",
    "fc1_weight = state_dict['fc1.weight']\n",
    "fc1_output_size, fc1_input_size = fc1_weight.size()\n",
    "\n",
    "fc2_weight = state_dict['fc2.weight']\n",
    "fc2_output_size, fc2_input_size = fc2_weight.size()\n",
    "\n",
    "with tempfile.NamedTemporaryFile(\n",
    "    delete=True, mode=\"w+\", newline=\"\", suffix=\".csv\"\n",
    ") as tmpfile:\n",
    "    csv_name = tmpfile.name\n",
    "    writer = csv.writer(tmpfile)\n",
    "\n",
    "    # Input nodes (1 channel for now)\n",
    "    for y in range(0, input_size):\n",
    "        for x in range(0, input_size):\n",
    "            name = f\"input.{x}.{y}\"\n",
    "            writer.writerow([node_idx, 0, name])\n",
    "            node_idx += 1\n",
    "            nodes[name] = node_idx\n",
    "\n",
    "    # Conv1\n",
    "    for y in range(0, conv1_output_size):\n",
    "        for x in range(0, conv1_output_size):\n",
    "            for c in range(0, conv1_out_channels):\n",
    "                name = f\"conv1.{c}.{x}.{y}\"\n",
    "                # The bias of this layer is simply the bias of the corresponding\n",
    "                # kernel\n",
    "                bias = conv1_bias[c]\n",
    "\n",
    "                writer.writerow([node_idx, bias.item(), name])\n",
    "                node_idx += 1\n",
    "                nodes[name] = node_idx\n",
    "\n",
    "    # Conv2\n",
    "    for y in range(0, conv2_output_size):\n",
    "        for x in range(0, conv2_output_size):\n",
    "            for c in range(0, conv2_out_channels):\n",
    "                name = f\"conv2.{c}.{x}.{y}\"\n",
    "                bias = conv2_bias[c]\n",
    "\n",
    "                writer.writerow([node_idx, bias.item(), name])\n",
    "                node_idx += 1\n",
    "                nodes[name] = node_idx\n",
    "\n",
    "    # fc1\n",
    "    for i in range(0, fc1_output_size):\n",
    "        name = f\"fc1.{i}\"\n",
    "        bias = state_dict['fc1.bias'][i]\n",
    "        writer.writerow([node_idx, bias.item(), name])\n",
    "        node_idx += 1\n",
    "        nodes[name] = node_idx\n",
    "\n",
    "    # fc2\n",
    "    for i in range(0, fc2_output_size):\n",
    "        name = f\"fc2.{i}\"\n",
    "        bias = state_dict['fc2.bias'][i]\n",
    "        writer.writerow([node_idx, bias.item(), name])\n",
    "        node_idx += 1\n",
    "        nodes[name] = node_idx\n",
    "\n",
    "    # Flush\n",
    "    tmpfile.flush()\n",
    "    tmpfile.seek(0)\n",
    "    cursor.copy_from(tmpfile, 'node', sep=',')\n",
    "\n",
    "with tempfile.NamedTemporaryFile(\n",
    "    delete=True, mode=\"w+\", newline=\"\", suffix=\".csv\"\n",
    ") as tmpfile:\n",
    "    csv_name = tmpfile.name\n",
    "    writer = csv.writer(tmpfile)\n",
    "\n",
    "    # Add the edges from input to conv1. Per channel, per output pixel of the\n",
    "    # convolution, we have to match the 9 input pixels to it (for a 3x3 kernel)\n",
    "    for c in range(0, conv1_out_channels):\n",
    "        for y_conv in range(0, conv1_output_size):\n",
    "            for x_conv in range(0, conv1_output_size):\n",
    "                # (x_conv, y_conv) is the position in the output channel. We can\n",
    "                # find the 9 matching input values from them.\n",
    "                for (p_in, p_kernel) in get_contributing_points((x_conv, y_conv), conv1_kernel_size):\n",
    "                    (x_in, y_in) = p_in\n",
    "                    (x_kernel, y_kernel) = p_kernel\n",
    "\n",
    "                    # 0 corresponds to the input channel (which we only have one\n",
    "                    # of).\n",
    "                    kernel = conv1_weight[c][0]\n",
    "                    weight = kernel[y_kernel][x_kernel]\n",
    "\n",
    "                    src = nodes[f\"input.{x_in}.{y_in}\"]\n",
    "                    dst = nodes[f\"conv1.{c}.{x_conv}.{y_conv}\"]\n",
    "\n",
    "                    writer.writerow([src, dst, weight.item()])\n",
    "\n",
    "\n",
    "    # Add the edges from conv1 to conv2. This is similar as connecting the input to\n",
    "    # conv1, except that we have 2 input channels. Outputs are summed per output\n",
    "    # channel.\n",
    "    for c_out in range(0, conv2_out_channels):\n",
    "        for y_conv2 in range(0, conv2_output_size):\n",
    "            for x_conv2 in range(0, conv2_output_size):\n",
    "                for (p_in, p_kernel) in get_contributing_points((x_conv2, y_conv2), conv2_kernel_size):\n",
    "                    (x_in, y_in) = p_in\n",
    "                    (x_kernel, y_kernel) = p_kernel\n",
    "\n",
    "                    for c_in in range(0, conv2_in_channels):\n",
    "                        kernel = conv2_weight[c_out][c_in]\n",
    "                        weight = kernel[y_kernel][x_kernel]\n",
    "\n",
    "                        src = nodes[f\"conv1.{c_in}.{x_in}.{y_in}\"]\n",
    "                        dst = nodes[f\"conv2.{c_out}.{x_conv2}.{y_conv2}\"]\n",
    "\n",
    "                        writer.writerow([src, dst, weight.item()])\n",
    "\n",
    "    # Connect conv2 to fc1.\n",
    "    for c in range(0, conv2_out_channels):\n",
    "        for y_conv in range(0, conv2_output_size):\n",
    "            for x_conv in range(0, conv2_output_size):\n",
    "                for i in range(0, fc1_output_size):\n",
    "                    # By adding the channel offset, we flatten.\n",
    "                    channel_offset = c * conv2_output_size * conv2_output_size\n",
    "                    weight = fc1_weight[i][y_conv * conv2_output_size + x_conv + channel_offset]\n",
    "\n",
    "                    src = nodes[f\"conv2.{c}.{x_conv}.{y_conv}\"]\n",
    "                    dst = nodes[f\"fc1.{i}\"]\n",
    "\n",
    "                    writer.writerow([src, dst, weight.item()])\n",
    "\n",
    "    # Connect fc1 to fc2.\n",
    "    for i in range(0, fc2_input_size):\n",
    "        for j in range(0, fc2_output_size):\n",
    "            weight = fc2_weight[j][i]\n",
    "\n",
    "            src = nodes[f\"fc1.{i}\"]\n",
    "            dst = nodes[f\"fc2.{j}\"]\n",
    "\n",
    "\n",
    "            writer.writerow([src, dst, weight.item()])\n",
    "\n",
    "    tmpfile.flush()\n",
    "    tmpfile.seek(0)\n",
    "    cursor.copy_from(tmpfile, 'edge', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see how it performs on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAANgUlEQVR4nO3cW4iV5RrA8Wc5moqBqDhgkZodSCHJNJUaaazIKbsYUYIKwpsJSkKI7AClBkEYHcQMEyosnIhKk0ixINMuMs0OkqJ5KCstj1OphZq49sVmP9R22nu+1Yzj6O8H3nx8z/redbP+vmtm3lK5XC4HAEREp/ZeAACnD1EAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFHgjLRjx44olUrx1FNPtdprrly5MkqlUqxcubLVXhNON6LAaWPBggVRKpVi3bp17b2UNjFw4MAolUrN/rvkkkvae3kQERGd23sBcLaYPXt2HD58+C/Xvvvuu3jkkUfixhtvbKdVwV+JApwi9fX1J117/PHHIyLijjvuOMWrgeb5+ogO5dixYzF9+vQYPnx49OzZM3r06BFjxoyJDz/88G9nnn322RgwYEB07949rr322tiwYcNJ92zevDkmTZoUvXv3jm7dusWIESPinXfe+b/r+f3332Pz5s2xf//+it7Pa6+9FhdeeGFcffXVFc1DaxMFOpSDBw/Giy++GLW1tTFr1qyYOXNm7Nu3L8aNGxdffvnlSfe/+uqrMWfOnJgyZUo8/PDDsWHDhrjuuutiz549ec/GjRtj9OjRsWnTpnjooYfi6aefjh49ekR9fX28/fbb/3M9a9eujcGDB8fcuXMLv5cvvvgiNm3aFLfffnvhWWgrvj6iQ+nVq1fs2LEjzjnnnLzW0NAQl112WTz33HPx0ksv/eX+bdu2xdatW+P888+PiIi6uroYNWpUzJo1K5555pmIiJg6dWr0798/Pv300+jatWtERNxzzz1RU1MTDz74YEyYMKFN3ktjY2NE+OqI04udAh1KVVVVBuHEiRPR1NQUx48fjxEjRsTnn39+0v319fUZhIiIkSNHxqhRo2LZsmUREdHU1BQrVqyIW2+9NQ4dOhT79++P/fv3x4EDB2LcuHGxdevW2LVr19+up7a2NsrlcsycObPQ+zhx4kS8/vrrMWzYsBg8eHChWWhLokCH88orr8TQoUOjW7du0adPn+jbt28sXbo0fv3115Pube5XPS+99NLYsWNHRPx7J1Eul+PRRx+Nvn37/uXfjBkzIiJi7969rf4eVq1aFbt27bJL4LTj6yM6lIULF8bkyZOjvr4+pk2bFtXV1VFVVRVPPPFEbN++vfDrnThxIiIi7r///hg3blyz91x88cX/aM3NaWxsjE6dOsVtt93W6q8N/4Qo0KG89dZbMWjQoFi8eHGUSqW8/p//1f+3rVu3nnRty5YtMXDgwIiIGDRoUEREdOnSJW644YbWX3Azjh49GosWLYra2to477zzTskzoaV8fUSHUlVVFRER5XI5r61ZsyZWr17d7P1Lliz5y88E1q5dG2vWrImbbropIiKqq6ujtrY25s+fHz/99NNJ8/v27fuf66nkV1KXLVsWv/zyi6+OOC3ZKXDaefnll2P58uUnXZ86dWrccsstsXjx4pgwYUKMHz8+vv3223jhhRdiyJAhJ/21cMS/v/qpqamJu+++O44ePRqzZ8+OPn36xAMPPJD3PP/881FTUxOXX355NDQ0xKBBg2LPnj2xevXq2LlzZ6xfv/5v17p27doYO3ZszJgxo8U/bG5sbIyuXbvGxIkTW3Q/nEqiwGln3rx5zV6fPHlyTJ48OXbv3h3z58+P9957L4YMGRILFy6MN998s9mD6u68887o1KlTzJ49O/bu3RsjR46MuXPnRr9+/fKeIUOGxLp16+Kxxx6LBQsWxIEDB6K6ujqGDRsW06dPb9X3dvDgwVi6dGmMHz8+evbs2aqvDa2hVP7zPhyAs5qfKQCQRAGAJAoAJFEAIIkCAEkUAEgt/juFPx8pAEDH05K/QLBTACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIndt7AWeDSZMmFZ5paGio6Fk//vhj4ZkjR44UnmlsbCw8s3v37sIzERHbtm2raA4ozk4BgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIpXK5XG7RjaVSW6/ljPXNN98Unhk4cGDrL6SdHTp0qKK5jRs3tvJKaG07d+4sPPPkk09W9Kx169ZVNEdESz7u7RQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJA6t/cCzgYNDQ2FZ4YOHVrRszZt2lR4ZvDgwYVnrrzyysIztbW1hWciIkaPHl145ocffig8c8EFFxSeOZWOHz9eeGbfvn2FZ/r161d4phLff/99RXMOxGtbdgoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEilcrlcbtGNpVJbr4UzXK9evSqau+KKKwrPfPbZZ4VnrrrqqsIzp9KRI0cKz2zZsqXwTCWHKvbu3bvwzJQpUwrPRETMmzevojkiWvJxb6cAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYDkQDw4g02cOLHwzBtvvFF4ZsOGDYVnxo4dW3gmIqKpqamiORyIB0BBogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgOSUVOggqqurC8989dVXp+Q5kyZNKjyzaNGiwjP8M05JBaAQUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASJ3bewFAy0yZMqXwTN++fQvP/Pzzz4Vnvv7668IznJ7sFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkErlcrncohtLpbZeC5wVrrnmmormVqxYUXimS5cuhWdqa2sLz3z00UeFZzj1WvJxb6cAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYDUub0XAGebm2++uaK5Sg63++CDDwrPrF69uvAMZw47BQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJAfiwT/QvXv3wjN1dXUVPevYsWOFZ2bMmFF45o8//ig8w5nDTgGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEhOSYV/YNq0aYVnhg0bVtGzli9fXnjm448/ruhZnL3sFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkErlcrncohtLpbZeC7Sr8ePHF55ZsmRJ4Znffvut8ExERF1dXeGZTz75pKJncWZqyce9nQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAFLn9l4AtIU+ffoUnpkzZ07hmaqqqsIzy5YtKzwT4XA7Tg07BQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoApFK5XC636MZSqa3XAs2q5NC5Sg6PGz58eOGZ7du3F56pq6srPFPps+DPWvJxb6cAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYDUub0XAP/PRRddVHimksPtKnHfffcVnnGwHaczOwUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACA5JZVTZsCAARXNvf/++628kuZNmzat8My7777bBiuB9mOnAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGA5EA8Tpm77rqrorn+/fu38kqat2rVqsIz5XK5DVYC7cdOAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAyYF4VKSmpqbwzL333tsGKwFak50CAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSA/GoyJgxYwrPnHvuuW2wkuZt37698Mzhw4fbYCXQsdgpAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAySmpnPbWr19feOb6668vPNPU1FR4Bs40dgoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEilcrlcbtGNpVJbrwWANtSSj3s7BQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoApM4tvbGF5+YB0IHZKQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQ/gWd1HhaBfHXfAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset = datasets.MNIST('../data', train=False)\n",
    "image, label = dataset[0]\n",
    "\n",
    "image = transforms.ToTensor()(image)\n",
    "image_np = image.squeeze().numpy()\n",
    "\n",
    "plt.imshow(image_np, cmap='gray')\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRecursion",
     "evalue": "aggregate functions are not allowed in a recursive query's recursive term\nLINE 45:             n.bias + SUM(e.weight * tx.value)\n                              ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRecursion\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mqueries/saliency.sql\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     12\u001b[0m     saliency_query \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[0;32m---> 14\u001b[0m cursor\u001b[39m.\u001b[39;49mexecute(saliency_query)\n\u001b[1;32m     15\u001b[0m cursor\u001b[39m.\u001b[39mfetchall()\n",
      "\u001b[0;31mInvalidRecursion\u001b[0m: aggregate functions are not allowed in a recursive query's recursive term\nLINE 45:             n.bias + SUM(e.weight * tx.value)\n                              ^\n"
     ]
    }
   ],
   "source": [
    "def load_image_into_input_table(image):\n",
    "    cursor.execute(\"TRUNCATE input\")\n",
    "    for i, pixel in enumerate(image.flatten()):\n",
    "        cursor.execute(f\"\"\"\n",
    "            INSERT INTO input (input_set_id, input_node_idx, input_value)\n",
    "            VALUES (0, {i + 1}, {pixel.item()})\n",
    "        \"\"\")\n",
    "\n",
    "load_image_into_input_table(image)\n",
    "\n",
    "with open('queries/saliency.sql') as file:\n",
    "    saliency_query = file.read()\n",
    "\n",
    "cursor.execute(saliency_query)\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
