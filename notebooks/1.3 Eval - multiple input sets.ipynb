{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple input sets\n",
    "\n",
    "In the [previous chapter](./1.2%20Eval%20-%20multiple%20i-o.ipynb) we\n",
    "constructed a SQL query that can evaluate a neural network with multiple input\n",
    "and output neurons. This works nicely, but the query can only handle one set of\n",
    "input values at a time. It would be nice if we could pass multiple input sets at\n",
    "once, as we can do with a PyTorch model. In theory, SQL would give us\n",
    "parallelization for free then.\n",
    "\n",
    "## The Neural Network\n",
    "\n",
    "We'll start from the same neural network as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import utils.sqlite as db\n",
    "import utils.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(223)\n",
    "\n",
    "def f(x, y):\n",
    "    return [2*x, 4*y]\n",
    "\n",
    "num_samples = 100\n",
    "x_train = torch.randn(num_samples, 2) * 100\n",
    "y_train = [f(x,y) for [x,y] in x_train]\n",
    "\n",
    "model = nn.ReLUFNN(input_size=2, output_size=2, hidden_size=2, num_hidden_layers=1)\n",
    "nn.train(model, x_train, y_train, save_path=\"models/eval_multiple_sets.pt\")\n",
    "db.load_pytorch_model_into_db(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The query\n",
    "\n",
    "We'll look at a query that handles two sets of input at the same time. The full\n",
    "query looks like this:\n",
    "\n",
    "```sql\n",
    "WITH input_values AS (\n",
    "    SELECT 0 AS input_set_id, 1 AS input_node_idx, ? AS input_value\n",
    "    UNION \n",
    "    SELECT 0 AS input_set_id, 2 AS input_node_idx, ? AS input_value\n",
    "    UNION\n",
    "    SELECT 1 AS input_set_id, 1 AS input_node_idx, ? AS input_value\n",
    "    UNION \n",
    "    SELECT 1 AS input_set_id, 2 AS input_node_idx, ? AS input_value\n",
    "),\n",
    "input_nodes AS (\n",
    "    SELECT\n",
    "        id,\n",
    "        bias,\n",
    "        ROW_NUMBER() OVER (ORDER BY id) AS input_node_idx\n",
    "    FROM node\n",
    "    WHERE id NOT IN\n",
    "    (\n",
    "        SELECT dst FROM edge\n",
    "    )\n",
    "),\n",
    "t1 AS (\n",
    "    SELECT\n",
    "        v.input_set_id AS input_set_id,\n",
    "        MAX(\n",
    "            0,\n",
    "            n.bias + SUM(e.weight * v.input_value)\n",
    "        ) AS t1,\n",
    "        e.dst AS id\n",
    "    FROM edge e\n",
    "    JOIN input_nodes i ON i.id = e.src\n",
    "    JOIN node n ON e.dst = n.id\n",
    "    JOIN input_values v ON i.input_node_idx = v.input_node_idx\n",
    "    GROUP BY e.dst, n.bias, v.input_set_id\n",
    "),\n",
    "outputs AS (\n",
    "    SELECT\n",
    "        t1.input_set_id AS input_set_id,\n",
    "        n.bias + SUM(e.weight * t1.t1) AS output_value,\n",
    "        e.dst AS output_node_id\n",
    "    FROM edge e\n",
    "    JOIN t1 ON t1.id = e.src\n",
    "    JOIN node n ON e.dst = n.id\n",
    "    GROUP BY e.dst, n.bias, t1.input_set_id\n",
    ")\n",
    "SELECT * FROM outputs ORDER BY input_set_id, output_node_id;\n",
    "```\n",
    "\n",
    "The main difference with the previous iterations of the query is that we now add\n",
    "a numeric index `input_set_id` to the input values. This index is propagated\n",
    "through the $t_l$ queries and added to each aggregation. The result is a\n",
    "relation with each input set and their corresponding output values.\n",
    "\n",
    "The following code constructs this query by checking how many input sets are\n",
    "passed and how many layers are present in the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input value is a list of input value sets\n",
    "def eval_nn(model, input_value):\n",
    "    num_layers = int(len(model.state_dict()) / 2)\n",
    "\n",
    "    input_clauses = []\n",
    "    for input_set, input in enumerate(input_value):\n",
    "        for i,_ in enumerate(input):\n",
    "            input_clauses.append(f\"\"\"\n",
    "                SELECT\n",
    "                    {input_set} AS input_set_id,\n",
    "                    {i + 1} AS input_node_idx,\n",
    "                    ? AS input_value\n",
    "            \"\"\")\n",
    "\n",
    "    query = f\"\"\"\n",
    "        WITH input_values AS (\n",
    "            {\" UNION \".join(input_clauses)}\n",
    "        ),\n",
    "        input_nodes AS (\n",
    "            SELECT\n",
    "                id,\n",
    "                bias,\n",
    "                ROW_NUMBER() OVER (ORDER BY id) AS input_node_idx\n",
    "            FROM node\n",
    "            WHERE id NOT IN\n",
    "            (\n",
    "                SELECT dst FROM edge\n",
    "            )\n",
    "        ),\n",
    "        t1 AS (\n",
    "            SELECT\n",
    "                v.input_set_id AS input_set_id,\n",
    "                MAX(\n",
    "                    0,\n",
    "                    n.bias + SUM(e.weight * v.input_value)\n",
    "                ) AS t1,\n",
    "                e.dst AS id\n",
    "            FROM edge e\n",
    "            JOIN input_nodes i ON i.id = e.src\n",
    "            JOIN node n ON e.dst = n.id\n",
    "            JOIN input_values v ON i.input_node_idx = v.input_node_idx\n",
    "            GROUP BY e.dst, n.bias, v.input_set_id\n",
    "        ),\n",
    "        \"\"\"\n",
    "\n",
    "    for hidden_layer in range(2, num_layers):\n",
    "        curr = hidden_layer\n",
    "        prev = hidden_layer - 1\n",
    "        query += f\"\"\"\n",
    "            t{curr} AS (\n",
    "                SELECT\n",
    "                    t{prev}.input_set_id AS input_set_id,\n",
    "                    MAX(\n",
    "                        0,\n",
    "                        n.bias + SUM(e.weight * t{prev}.t{prev})\n",
    "                    ) AS t{curr},\n",
    "                    e.dst AS id\n",
    "                FROM edge e\n",
    "                JOIN t{prev} ON t{prev}.id = e.src\n",
    "                JOIN node n ON e.dst = n.id\n",
    "                GROUP BY e.dst, n.bias, t{prev}.input_set_id\n",
    "            ),\n",
    "        \"\"\"\n",
    "\n",
    "    prev = num_layers - 1\n",
    "    query += f\"\"\"\n",
    "        outputs AS (\n",
    "            SELECT\n",
    "                t{prev}.input_set_id AS input_set_id,\n",
    "                n.bias + SUM(e.weight * t{prev}.t{prev}) AS output_value,\n",
    "                e.dst AS output_node_id\n",
    "            FROM edge e\n",
    "            JOIN t{prev} ON t{prev}.id = e.src\n",
    "            JOIN node n ON e.dst = n.id\n",
    "            GROUP BY e.dst, n.bias, t{prev}.input_set_id\n",
    "        )\n",
    "        SELECT * FROM outputs ORDER BY input_set_id, output_node_id;\n",
    "    \"\"\"\n",
    "\n",
    "    args = []\n",
    "    for input_set in input_value:\n",
    "        for value in input_set:\n",
    "            args.append(value)\n",
    "\n",
    "    results = [[] for _ in range(0, len(input_value))]\n",
    "    for row in db.con.execute(query, args).fetchall():\n",
    "        (input_set_id, output, output_node_id) = row\n",
    "        results[input_set_id].append(output)\n",
    "\n",
    "    return np.array(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this query to pass multiple input sets at the same time, The\n",
    "query's output contains all output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The neural network predicted [[-13.467458  34.828564]\n",
      " [-24.508732  54.89358 ]]\n",
      "The SQL query calculated [[-13.46745742  34.82856474]\n",
      " [-24.50873458  54.89358546]]\n"
     ]
    }
   ],
   "source": [
    "nn_output = model(torch.tensor([[1, 10], [5,20]], dtype=torch.float32)).detach().numpy()\n",
    "sql_output = eval_nn(model, [[1, 10], [5, 20]])\n",
    "\n",
    "print(f\"The neural network predicted {nn_output}\")\n",
    "print(f\"The SQL query calculated {sql_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more layers\n",
    "\n",
    "As we did in the previous chapter, let's try it with a slightly larger model as\n",
    "well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigger_model = nn.ReLUFNN(input_size=2, output_size=2, hidden_size=10, num_hidden_layers=100)\n",
    "nn.train(bigger_model, x_train, y_train, save_path=\"models/eval_multiple_sets_bigger.pt\")\n",
    "db.load_pytorch_model_into_db(bigger_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the results again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The neural network predicted [[-33.277645 -32.772892]\n",
      " [-33.27765  -32.772896]]\n",
      "The SQL query calculated [[-33.27764824 -32.77289471]\n",
      " [-33.27764824 -32.77289471]]\n"
     ]
    }
   ],
   "source": [
    "nn_output = bigger_model(torch.tensor([[1, 10], [5,20]], dtype=torch.float32)).detach().numpy()\n",
    "sql_output = eval_nn(bigger_model, [[1, 10], [5, 20]])\n",
    "\n",
    "print(f\"The neural network predicted {nn_output}\")\n",
    "print(f\"The SQL query calculated {sql_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's compare a lot of input values at once. Note that we did\n",
    "something similar in the previous chapter, but there we had to pass multiple\n",
    "input sets in a for-loop. With our new implementation, the code for evaluating\n",
    "the model is almost identical to PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average difference for output value 1 is 1.9073486328125e-06\n",
      "The average difference for output value 2 is 1.9073486328125e-06\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['in_1', 'in_2', 'nn_out_1', 'nn_out_2', 'sql_out_1', 'sql_out_2'])\n",
    "\n",
    "nn_output = bigger_model(x_train).detach().numpy()\n",
    "sql_output = eval_nn(bigger_model, x_train.tolist())\n",
    "\n",
    "for i in range(0, len(x_train)):\n",
    "    df.loc[i] = [\n",
    "        x_train[i][0], x_train[i][1],\n",
    "        nn_output[i][0], nn_output[i][1],\n",
    "        sql_output[i][0], sql_output[i][1]\n",
    "    ]\n",
    "\n",
    "delta_1_avg = (abs(df['nn_out_1'] - df['sql_out_1'])).mean()\n",
    "delta_2_avg = (abs(df['nn_out_2'] - df['sql_out_2'])).mean()\n",
    "\n",
    "print(f\"The average difference for output value 1 is {delta_1_avg}\")\n",
    "print(f\"The average difference for output value 2 is {delta_2_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Now that our `eval` function in SQL is feature-complete, we'll finally take a\n",
    "look at getting rid of the repetition in the query in the next chapter.\n",
    "Specifically we'll use SQL's `WITH RECURSIVE` construct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
