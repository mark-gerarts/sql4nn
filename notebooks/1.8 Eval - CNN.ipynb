{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval on a CNN\n",
    "\n",
    "Up until now, we have always been running our `eval` query on basic ReluFNNs.\n",
    "Another popular type of networks are convolutional neural networks, which are\n",
    "especially effective in handling images. In this notebook, we'll show how we can\n",
    "transform a convolutional layer to an extra hidden layer. This makes it possible\n",
    "to run our `eval` query on it, without any alterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network\n",
    "\n",
    "We'll use [PyTorch's MNIST\n",
    "example](https://github.com/pytorch/examples/blob/main/mnist/main.py) as the CNN\n",
    "of our choice. The following code is taken directly from the example, with a few\n",
    "adaptations:\n",
    "\n",
    "1. The PyTorch example is written to be a CLI app. We remove the CLI part and\n",
    "   simply construct the network in code.\n",
    "2. The example uses a `max_pool2d` layer. Our work currently does not support\n",
    "   this, so we omit this layer. A pooling layer reduces the number of neurons,\n",
    "   so we adjust the number of `conv2` output channels accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/main/mnist/main.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import networkx as nx\n",
    "import utils.duckdb as db\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        # We divide by 4 here to adjust for the removed max_pool2d layer.\n",
    "        self.conv2 = nn.Conv2d(32, 64 // 4, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        # We removing the pooling layer since we can't implement it yet.\n",
    "        # x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 5-6mins on a decent machine.\n",
    "def load_or_train_model():\n",
    "    save_path = \"models/mnist_cnn.pt\"\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        model = Net()\n",
    "        model.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "        model.eval()\n",
    "\n",
    "        return model\n",
    "\n",
    "    # These are the default values for the CLI app.\n",
    "    class args():\n",
    "        epochs = 14\n",
    "        torch.manual_seed(1)\n",
    "        device = torch.device(\"cpu\")\n",
    "        batch_size = 64\n",
    "        learning_rate = 1.0\n",
    "        gamma = 0.7\n",
    "        log_interval = 10\n",
    "        dry_run = False\n",
    "\n",
    "    args = args()\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                        transform=transform)\n",
    "    dataset2 = datasets.MNIST('../data', train=False,\n",
    "                        transform=transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, batch_size=args.batch_size)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, batch_size=args.batch_size)\n",
    "\n",
    "    model = Net().to(args.device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, args.device, train_loader, optimizer, epoch)\n",
    "        test(model, args.device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = load_or_train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model trained and ready, we'll go on and insert it in the database. The\n",
    "difficult part is to translate the convolutions into a \"regular\" hidden layer.\n",
    "\n",
    "Say we have a 3x3 kernel. In a convolution, this kernel is moved in a sliding\n",
    "window over the input matrix. In each step, the input pixels are multiplied\n",
    "element-wise with the kernel's corresponding weight value and then summed. We\n",
    "can simulate this behavior by connecting each group of 9 input nodes to the\n",
    "corresponding output node, and setting the kernel weights as the weight for each\n",
    "edge. The summing is part of our regular model evaluation.\n",
    "\n",
    "The following function is a utility function to find out which input points are\n",
    "responsible for a given output point after the convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contributing_points(output_point, kernel_size, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    Given an output point (of the image after convolution), calculates which\n",
    "    points of the input image contributed. They are returned, along with the\n",
    "    corresponding kernel location.\n",
    "    \"\"\"\n",
    "\n",
    "    x_out, y_out = output_point\n",
    "    x_in_start = x_out * stride - padding\n",
    "    y_in_start = y_out * stride - padding\n",
    "\n",
    "    contributing_points = []\n",
    "    for i in range(kernel_size):\n",
    "        for j in range(kernel_size):\n",
    "            x_in = x_in_start + i\n",
    "            y_in = y_in_start + j\n",
    "            contributing_points.append(((x_in, y_in), (i, j)))\n",
    "\n",
    "    return contributing_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code constructs the network in the database. It is tedious to\n",
    "translate the convolutions, because the model also uses multiple channels for\n",
    "the convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing existing database dbs/eval_cnn.db\n"
     ]
    }
   ],
   "source": [
    "def load_or_create_database(model):\n",
    "    save_path = \"dbs/eval_cnn.db\"\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        db.reconnect()\n",
    "        db.con.execute(f\"IMPORT DATABASE '{save_path}'\")\n",
    "        print(f\"Importing existing database {save_path}\")\n",
    "        return\n",
    "\n",
    "    db._initialize_database()\n",
    "\n",
    "    # We have to hardcode this\n",
    "    input_size = 28\n",
    "\n",
    "    state_dict = model.state_dict()\n",
    "    _, conv1_kernel_size, _ = state_dict['conv1.weight'][0].size()\n",
    "\n",
    "    node_idx = 1\n",
    "    nodes = {}\n",
    "\n",
    "    conv1_weight = state_dict['conv1.weight']\n",
    "    conv1_bias = state_dict['conv1.bias']\n",
    "    conv1_output_size = input_size - conv1_kernel_size + 1\n",
    "\n",
    "    conv1_out_channels, conv1_in_channels, conv1_kernel_size, _ = conv1_weight.size()\n",
    "    if conv1_in_channels != 1:\n",
    "        raise Exception(\"Not handling >1 input channels for now\")\n",
    "\n",
    "    conv2_weight = state_dict['conv2.weight']\n",
    "    conv2_bias = state_dict['conv2.bias']\n",
    "\n",
    "    conv2_out_channels, conv2_in_channels, conv2_kernel_size, _ = conv2_weight.size()\n",
    "    conv2_output_size = conv1_output_size - conv2_kernel_size + 1\n",
    "\n",
    "    fc1_weight = state_dict['fc1.weight']\n",
    "    fc1_output_size, fc1_input_size = fc1_weight.size()\n",
    "\n",
    "    fc2_weight = state_dict['fc2.weight']\n",
    "    fc2_output_size, fc2_input_size = fc2_weight.size()\n",
    "\n",
    "    def node_generator(nodes):\n",
    "        node_idx = 1\n",
    "\n",
    "        # Input nodes (1 channel for now)\n",
    "        for y in range(0, input_size):\n",
    "            for x in range(0, input_size):\n",
    "                name = f\"input.{x}.{y}\"\n",
    "                yield [node_idx, 0, name]\n",
    "                nodes[name] = node_idx\n",
    "                node_idx += 1\n",
    "\n",
    "        # Conv1\n",
    "        for y in range(0, conv1_output_size):\n",
    "            for x in range(0, conv1_output_size):\n",
    "                for c in range(0, conv1_out_channels):\n",
    "                    name = f\"conv1.{c}.{x}.{y}\"\n",
    "                    # The bias of this layer is simply the bias of the corresponding\n",
    "                    # kernel\n",
    "                    bias = conv1_bias[c]\n",
    "\n",
    "                    yield [node_idx, bias.item(), name]\n",
    "                    nodes[name] = node_idx\n",
    "                    node_idx += 1\n",
    "\n",
    "        # Conv2\n",
    "        for y in range(0, conv2_output_size):\n",
    "            for x in range(0, conv2_output_size):\n",
    "                for c in range(0, conv2_out_channels):\n",
    "                    name = f\"conv2.{c}.{x}.{y}\"\n",
    "                    bias = conv2_bias[c]\n",
    "\n",
    "                    yield [node_idx, bias.item(), name]\n",
    "                    nodes[name] = node_idx\n",
    "                    node_idx += 1\n",
    "\n",
    "        # fc1\n",
    "        for i in range(0, fc1_output_size):\n",
    "            name = f\"fc1.{i}\"\n",
    "            bias = state_dict['fc1.bias'][i]\n",
    "            yield [node_idx, bias.item(), name]\n",
    "            nodes[name] = node_idx\n",
    "            node_idx += 1\n",
    "\n",
    "        # fc2\n",
    "        for i in range(0, fc2_output_size):\n",
    "            name = f\"fc2.{i}\"\n",
    "            bias = state_dict['fc2.bias'][i]\n",
    "            yield [node_idx, bias.item(), name]\n",
    "            nodes[name] = node_idx\n",
    "            node_idx += 1\n",
    "\n",
    "    def edge_generator(nodes):\n",
    "        # Add the edges from input to conv1. Per channel, per output pixel of the\n",
    "        # convolution, we have to match the 9 input pixels to it (for a 3x3 kernel)\n",
    "        for c in range(0, conv1_out_channels):\n",
    "            for y_conv in range(0, conv1_output_size):\n",
    "                for x_conv in range(0, conv1_output_size):\n",
    "                    # (x_conv, y_conv) is the position in the output channel. We can\n",
    "                    # find the 9 matching input values from them.\n",
    "                    for (p_in, p_kernel) in get_contributing_points((x_conv, y_conv), conv1_kernel_size):\n",
    "                        (x_in, y_in) = p_in\n",
    "                        (x_kernel, y_kernel) = p_kernel\n",
    "\n",
    "                        # 0 corresponds to the input channel (which we only have one\n",
    "                        # of).\n",
    "                        kernel = conv1_weight[c][0]\n",
    "                        weight = kernel[y_kernel][x_kernel]\n",
    "\n",
    "                        src = nodes[f\"input.{x_in}.{y_in}\"]\n",
    "                        dst = nodes[f\"conv1.{c}.{x_conv}.{y_conv}\"]\n",
    "\n",
    "                        yield [src, dst, weight.item()]\n",
    "\n",
    "\n",
    "        # Add the edges from conv1 to conv2. This is similar as connecting the input to\n",
    "        # conv1, except that we have 2 input channels. Outputs are summed per output\n",
    "        # channel.\n",
    "        for c_out in range(0, conv2_out_channels):\n",
    "            for y_conv2 in range(0, conv2_output_size):\n",
    "                for x_conv2 in range(0, conv2_output_size):\n",
    "                    for (p_in, p_kernel) in get_contributing_points((x_conv2, y_conv2), conv2_kernel_size):\n",
    "                        (x_in, y_in) = p_in\n",
    "                        (x_kernel, y_kernel) = p_kernel\n",
    "\n",
    "                        for c_in in range(0, conv2_in_channels):\n",
    "                            kernel = conv2_weight[c_out][c_in]\n",
    "                            weight = kernel[y_kernel][x_kernel]\n",
    "\n",
    "                            src = nodes[f\"conv1.{c_in}.{x_in}.{y_in}\"]\n",
    "                            dst = nodes[f\"conv2.{c_out}.{x_conv2}.{y_conv2}\"]\n",
    "\n",
    "                            yield [src, dst, weight.item()]\n",
    "\n",
    "        # Connect conv2 to fc1.\n",
    "        for c in range(0, conv2_out_channels):\n",
    "            for y_conv in range(0, conv2_output_size):\n",
    "                for x_conv in range(0, conv2_output_size):\n",
    "                    for i in range(0, fc1_output_size):\n",
    "                        # By adding the channel offset, we flatten.\n",
    "                        channel_offset = c * conv2_output_size * conv2_output_size\n",
    "                        weight = fc1_weight[i][y_conv * conv2_output_size + x_conv + channel_offset]\n",
    "\n",
    "                        src = nodes[f\"conv2.{c}.{x_conv}.{y_conv}\"]\n",
    "                        dst = nodes[f\"fc1.{i}\"]\n",
    "\n",
    "                        yield [src, dst, weight.item()]\n",
    "\n",
    "        # Connect fc1 to fc2.\n",
    "        for i in range(0, fc2_input_size):\n",
    "            for j in range(0, fc2_output_size):\n",
    "                weight = fc2_weight[j][i]\n",
    "\n",
    "                src = nodes[f\"fc1.{i}\"]\n",
    "                dst = nodes[f\"fc2.{j}\"]\n",
    "\n",
    "                yield [src, dst, weight.item()]\n",
    "\n",
    "    db.batch_insert(node_generator(nodes), \"node\")\n",
    "    db.batch_insert(edge_generator(nodes), \"edge\")\n",
    "\n",
    "    db.con.execute(f\"EXPORT DATABASE '{save_path}'\")\n",
    "    print(f\"Exported to {save_path}\")\n",
    "\n",
    "load_or_create_database(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place, we can create a function that inserts a given input image in\n",
    "the `input` table in the database. This table is used in the `eval` query to\n",
    "fetch the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_input_table(image):\n",
    "    db.con.execute(\"TRUNCATE input\")\n",
    "    for i, pixel in enumerate(image.flatten()):\n",
    "        db.con.execute(\"\"\"\n",
    "            INSERT INTO input (input_set_id, input_node_idx, input_value)\n",
    "            VALUES (0, $input_node_idx, $input_value)\n",
    "        \"\"\",\n",
    "        {'input_node_idx': i + 1, 'input_value': pixel.item()})\n",
    "    db.con.execute(f\"EXPORT DATABASE 'dbs/eval_cnn_w_input.db'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `eval` query we use is the recursive eval query from an [earlier\n",
    "notebook](./1.4%20Eval%20-%20recursive.ipynb) with a slight addition: after\n",
    "calculating the output values, we also perform a log softmax in SQL:\n",
    "\n",
    "```sql\n",
    "WITH\n",
    "-- Original eval query is ommitted for brevity.\n",
    "max_value AS (\n",
    "    SELECT\n",
    "        input_set_id,\n",
    "        MAX(value) AS max_val\n",
    "    FROM t_out\n",
    "    GROUP BY input_set_id\n",
    "),\n",
    "log_sum_exp AS (\n",
    "    SELECT\n",
    "        t.input_set_id,\n",
    "        LOG(SUM(EXP(t.value - m.max_val))) AS log_sum_exp\n",
    "    FROM t_out t\n",
    "    JOIN max_value m ON t.input_set_id = m.input_set_id\n",
    "    GROUP BY t.input_set_id\n",
    ")\n",
    "SELECT\n",
    "    t.input_set_id,\n",
    "    t.id,\n",
    "    t.value - m.max_val - lse.log_sum_exp AS log_softmax\n",
    "FROM t_out t\n",
    "JOIN max_value m ON t.input_set_id = m.input_set_id\n",
    "JOIN log_sum_exp lse ON t.input_set_id = lse.input_set_id\n",
    "ORDER BY t.id;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full query is available [here](./queries/eval_recursive_from_input_with_softmax.sql)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"queries/eval_recursive_from_input_with_softmax.sql\", \"r\") as file:\n",
    "    query = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick an image from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.MNIST('../data', train=False)\n",
    "image, label = dataset[0]\n",
    "image = transforms.ToTensor()(image)\n",
    "image_np = image.squeeze().numpy()\n",
    "\n",
    "plt.imshow(image_np, cmap='gray')\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the model's result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-28.4653, -26.2044, -24.6023, -21.7036, -31.7144, -31.9441, -34.7408,\n",
       "           0.0000, -30.6441, -21.9295]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize it manually, cfr the transform of the dataset loader.\n",
    "image = (image - 0.1307) / 0.3081\n",
    "\n",
    "model(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the result of our query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────────┬───────┬────────────────────────┐\n",
       "│ input_set_id │  id   │      log_softmax       │\n",
       "│    int32     │ int32 │         double         │\n",
       "├──────────────┼───────┼────────────────────────┤\n",
       "│            0 │ 31761 │     -28.46535068095316 │\n",
       "│            0 │ 31762 │     -26.20436089898085 │\n",
       "│            0 │ 31763 │     -24.60228292734754 │\n",
       "│            0 │ 31764 │     -21.70356275675611 │\n",
       "│            0 │ 31765 │    -31.714450652769717 │\n",
       "│            0 │ 31766 │    -31.944062948848238 │\n",
       "│            0 │ 31767 │     -34.74080294490496 │\n",
       "│            0 │ 31768 │ -3.039444454216385e-10 │\n",
       "│            0 │ 31769 │    -30.644082454459028 │\n",
       "│            0 │ 31770 │     -21.92954830296085 │\n",
       "├──────────────┴───────┴────────────────────────┤\n",
       "│ 10 rows                             3 columns │\n",
       "└───────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_image_into_input_table(image)\n",
    "db.con.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the results match: both the PyTorch model and the database\n",
    "`eval` classify the image as a \"7\", with matching output weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
